#### Paging
Paging is a technique where real and virtual addressing spaces are divided in equal sized blocks.
The blocks in the physical memory (RAM) are called frames.
The blocks in the logical memory are called pages.

This technique avoid adjusting sizes in a dynamic size memory.

When a process must be executed, its pages are loaded to a available frame. The mapping of virtual address to real is made using the page tables, where each process has its own table. The MMU is responsible for the translation
Each virtual page in the process has a entry in the table, with information about the mapping that allows the system to locate the real page.

Each virtual address is split into two parts: page number and offset inside the page.
The page number is used as an index of the page table, that possesses the base address of each frame in the physical memory.
The real address is obtained summing the frame address with the offset inside the page.

Example:
- p = page number
- d = page offset
- f = frame address

Logical memory of 16 bytes -> (16 bytes = 2^7 bits)
4 bytes pages -> (4 bytes = 2^5 bits)

7 total bits -> 5 bits for page offset -> 2 bits for page number


Physical memory of 32 bytes -> 2^8 bits
8 total bits -> 5 bits for page offset -> 3 bits for frame address

An entry o the page table has 3 bits and can reference only up to 2³ frames.

## Page Table 
Other than information about page location, the page table has information like:
- Valid Bit: If set, it tells us that the page is in the addressing space.
- Protection bit: It tells us what access is permitted in the page (read, write, execution).
- Modification bit: If set, tell us that page was written in the main memory.
- Reference bit: If set, tell us that the page was referenced in the main memory.

When a process references a virtual address, the MMU verifies through the valid bit if the page is in memory.
If the reference was made to a page that is not loaded to the main memory, than a page fault occurs. The system transfers the page from the secondary memory to the main memory.
The number of page faults generated by each process in a time interval is called pagination rate.
If pagination rate of processes are high, I/O operation in excess can compromise the system.

## Page Searching
By loading a page to memory, two strategies can be chosen: paging by demand and in advance.
#### By Demand 
Pages are transferred from the second memory  to main memory only when referenced.
It takes only demanded pages to the main memory.

#### In Advance
More pages can be loaded to the main memory when necessary. If the program is sequentially stored, we can load a set of pages.
If the program doesn't need the paging in advance, the system will have wasted time and memory.

### Both
A combined strategy can be used:
- The paging in advance technique can be employed on process creation.
- When the process is created, loading the pages lead to high number of page faults. But when executing the process the page fault rate lowers rapidly.


## Working Set
The main memory has pages of different processes.
During the execution of a process, some pages are loaded and other are swapped out.
When the system wastes too much time swapping pages, this is called thrashing.
To avoid thrashing is necessary to guess which part of the code is going to be necessary in the future - locality.

The working set of a process is the set of pages constantly referenced by the process.
The working set must: stay in the main memory to execute the process efficiently, must have a maximum limit of allowed pages.

If the working set is to big: Less page faults, Less process sharing the memory.

A way of implement a working set is to see the paging rate of each process.
If high: the limit of real pages has to be brought up to reach the working set.
If low: the system can lower the page limit.

## Paging Allocation Policies
Determines how many frames a process can have in the main memory.
#### Fixed Allocation
Each process has a max number of frames that can be used during execution.
If the frame limite is:
- Low -> High page faults rate
- High -> Less sharing, more process swapping
#### Variable Allocation
The number of frames allocated to a process during it's execution can vary.
If the process paging rate is:
- Low -> It's frames can be allocated to other processes.
- High -> Can set the max limit of frames higher to lower the paging rate.

## Page Substitution Policies
#### Local Scope
Only pages from the process that caused the page fault can be candidates to substitution.
#### Global Scope
All pages can be candidates to substitution.

There is a relation between the substitution and allocation policies.
**Fixed allocation** only allows local scope substitution.
**Variable allocation** allows both scopes:
- Local: In time intervals, the number of frames can be recalculated, but substitution are local.
- Global: The number of frames varies when process allocate frames from other processes.

The main difficulty in virtual memory managing is choosing which page will be taken out of the main memory.

## Page Substitution Algorithms
### OPT or MIN: Optimal Algorithm
Has the best fault page rate of all algorithms.
It substitutes the page that will not be reference for the longer time.
Hard to know in advance the reference string (order in which the pages are referenced).
As it produces the best result, is used in comparative studies to test algorithms efficiency.

Reference String: 
	7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1
Sequence:
9 page faults

| 7   | 7   | 7   | 2   | 2   | 2   | 2   | 2   | 7   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|     | 0   | 0   | 0   | 0   | 4   | 0   | 0   | 0   |
|     |     | 1   | 1   | 3   | 3   | 3   | 1   | 1   |

### FIFO (First In First Out)
When a page needs to substituted, the oldest page will be chosen to exit the memory.
Easy to program.
It's reasonably to think that a page was loaded to the MP for a time, it has been sufficiently  referenced.
- But, this is not always true and this can make the performance worst.
- There's a chance that the first page laoaded will be the most referenced.

Reference String: 
	7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1
Sequence:
15 page faults

| 7   | 7   | 7   | 2   | 2   | 2   | 4   | 4   | 4   | 0   | 0   | 0   | 7   | 7   | 7   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|     | 0   | 0   | 0   | 3   | 3   | 3   | 2   | 2   | 2   | 1   | 1   | 1   | 0   | 0   |
|     |     | 1   | 1   | 1   | 0   | 0   | 0   | 3   | 3   | 3   | 2   | 2   | 2   | 1   |

**Belady** Anomaly: the rate of page faults can be higher when the frame quantity rises.

### LRU (Least Recently Used)
Replaces a page that was last used. If we consider the temporal locality principle, it's good to guess that a page that was not referenced recently it might not be referenced in the near future.
Has overload because it must update the moment of access for each page. 

### NRU (Not Recently Used)
Uses a reference bit to show if the page was used recently.
Initially when a page is loaded to the memory, the reference bit is zero. When the page is referenced while in the memory the bit flips to 1, periodically the bit is flipped back. The algorithm replaces the page that has the bit set to 0.
It might be more efficient if we add more reference bits.

### Second Chance
It's a modified version of FIFO, with the objective of not replacing a page that is used frequently.
If the reference bit of the oldest page is 0, than the replacing procedes. If 1, than the page receives a second chance, its reference bit is set to 0 and its arrival restarted (goes to the end of the queue).
If a page is used frequently, it's bit is always 1, so never replaced.

### Clock Algorithm
Uses the second chance algorithm, but, instead of inserting a page that had a second chance to the end of the queue, the algorithm uses a circular queue.

#### Adicionar Reference Bits (NRU/Second Chance)
- The algorithm works with a reference bit and a modification bit (dirty bit), creating 4 classes of pages:
	- (0,0) not used recently, not modified (good page to replace)
	- (0,1) not used recently, but modified (not good, because we have to write it to disk)
	- (1,0) used recently, not modified (probably will be used)
	- (1,1) used, modified (not good to replace)

### Counting Based Algorithm
Number of references made are tracked to each page with a counter.
#### LFU (Least Frequently Used)
- Page with the lower count is replaced.
- Prioritizes frequently used pages.
- Recently loaded pages will have lower counters.
#### MFU (Most Frequently Used)
- Page with the higher count is replaced
- Based on the argument that the page with the lower count has just been loaded to the memory and is still being used.

### Page Size
Suppose virtual addressing of 32 bits
If a page has size 16 Kbytes (= 16384 bytes ) and each address in this page has 4 bytes. (16 Kb/ 4 B) = ( 2¹⁴ / 2²) = 2¹² addresses. 
We have 2¹² (4096) entries in the page.
So the virtual address is calculated like: 12 bits for offset + 20 bits to number it.
If we have 2²⁰ pages, we can multiply with the size of each page address (4 bytes), to find out about the size of the page table (2²⁰ * 4 bytes) = 4 Mb.
## Multilevel Paging
In single level paging systems, the size of the table can be a problem, for example: a system where processes resides in the main memory, it's hard and costly to manage 4 Mb tables.
A good solution is adding a new level to the page table.
Using 2 levels, we can have a directory level entry to each table.
Example:
The number of a page in level 1 (directory), allows us to find the address of a page table in the directory table.
The number of a page in level 2 (pages), allows us to find the address of the frame in the page table.
In both level we have an offset.

#### Single Level
2²⁰ entries in the page table, each entry with 4 bytes = 4 Mb.
Page table size = 4 Mb.
#### Two Levels
2¹⁰ entries in the directory table, each entry with 4 bytes = 4 Kb
Directory table size = 4 Kb

Paging in two levels is not the limit and can be extended.
The advantage of having multiple levels is that only the necessary page table are store in the main memory, reducing the allocated space.
To avoid a high paging rate, the ideia is using the locality principle in the other levels.

## TLB
Using mapping you have to access the main memory twice, one for accessing the page table and one for the physical address.
To speed up the process, we can use the page table with dedicated registers, but this can only work with smaller tables.

The TLB (Translation Lookaside Buffer) follows the locality principle, referencing less frames at a time, with this, only part of the table is necessary in the main memory.
The TLB is implemented like a cache memory, it only has the mapping of recently referenced virtual addresses. 
Each entry in the TLB has a key and a value.
The TLB stores only some entries in the page table.
When a logic address is generated by the CPU, it's page number is searched for in the TLB
- If is found, the frame number is available instantly.
- If not, a TLB miss happens, and the reference must be made in the page table.
